{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89a0428f50e9e3d64e07131f0679f71d",
     "grade": false,
     "grade_id": "cell-d8b377aba23d9f3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Mandatory Assignment 1\n",
    "\n",
    "This is the second of three mandatory assignments which must be completed during the course. Note that you only need to pass 2 out of 3 assignments to be eligible for the exam.\n",
    "\n",
    "First some practical pieces of information:\n",
    "\n",
    "* When is the assignment due?: **23:59, Friday, August 6, 2020.**\n",
    "* Should i work with my group?: **Yes**. In particular, you should **only hand in 1 assignment per group and in a comment on Absalon write your group number and all group members**. \n",
    "\n",
    "The assignment consists of problems from some of the exercise sets that you have solved so far. Some exercises are modified a little to better suit the structure of the assignment. \n",
    "\n",
    "**Note**: It is important that you submit your edited version of THIS [notebook](https://fileinfo.com/extension/ipynb#:~:text=An%20IPYNB%20file%20is%20a,Python%20language%20and%20their%20data.) (the one you have downloaded from Absalon) as a .ipynb file and nothing else. Do not copy your answers into another notebook that you have made. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bdaca21351127ed0e82dce43ad7d77b",
     "grade": false,
     "grade_id": "cell-e5576badd2b58d90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 2:\n",
    "\n",
    "We continue with the exercise that analyzes NOAA data. This time we are going to **read the weather data from a csv file** located in this assignment directory instead of trying to request the website. The file is called `'weather_data_1870-1875.csv'` and consists of weather data for the period 1870-1875. Specifically, the csv file contains a dataframe which has been constructed by concatenating the _non-processed_ data from 1870-1875."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27c3c562f2d291d26ca22c058a3f6fcf",
     "grade": false,
     "grade_id": "cell-3949fc8a0311b795",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 2.3.4:** The code below runs through some of the steps we completed in exercise 2.3.4 in Module 2. As we are not going to request the website but load the data from a csv file, your task is to **rewrite parts of the function**. In particular, you need to do the following:\n",
    ">1. Rename the function to `process_weather` instead of `load_weather`. \n",
    ">2. The function should now  take a `dataframe` as input. \n",
    ">3. Consider whether `df_weather.iloc[:, :4]` is necessary for the weather data loaded from  the csv file. The documentation string should also be rewritten appropriately. \n",
    ">4. The function contains a sorting step. **Change it so that it first sorts by _station_, then by _datetime_. The sorting should be ascending for _station_ and descending for _datetime_.** \n",
    ">5. After having rewritten the function, load the weather data from `'weather_data_1870-1875.csv'` into a pandas dataframe, apply the `process_weather` function to this dataframe, and store the result in the variable `df_weather_period`.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def load_weather(year):\n",
    "    \n",
    "    '''\n",
    "    This functions loads the data for selected year and then structures and cleans it.\n",
    "    - Structuring includes removing unused columns, renaming and selecting only observations \n",
    "    of maximum temperature. \n",
    "    - Cleaning includes inserting missing decimal, sorting and resetting index.\n",
    "    '''\n",
    "    url = f\"ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/{year}.csv.gz\"\n",
    "    \n",
    "    # loads the data\n",
    "    df_weather = pd.read_csv(url, header=None)\\\n",
    "                    .iloc[:,:4] \n",
    "    \n",
    "    # structure and clean data using methods chaining\n",
    "    # note that the original columns now are strings when loading the csv file\n",
    "    # and not integers as when downloading the data\n",
    "    df_out = \\\n",
    "        df_weather\\\n",
    "            .rename(columns={'0': 'station', '1': 'datetime', '2': 'obs_type', '3': 'obs_value'})\\\n",
    "            .query(\"obs_type == 'TMAX'\")\\\n",
    "            .assign(obs_value=lambda df: df['obs_value']/10)\\\n",
    "            .sort_values(by=['station', 'datetime'])\\\n",
    "            .reset_index(drop=True)\\\n",
    "            .copy() \n",
    "\n",
    "    # area process\n",
    "    df_out['area'] = df_out['station'].str[0:2]\n",
    "    \n",
    "    # datetime process\n",
    "    df_out['datetime_dt'] = pd.to_datetime(df_out['datetime'], format = '%Y%m%d')\n",
    "    df_out['month'] = df_out['datetime_dt'].dt.month\n",
    "    df_out['year'] = df_out['datetime_dt'].dt.year\n",
    "    \n",
    "    return df_out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0fcfb2b712a697a2c519e6f2d4102b6",
     "grade": false,
     "grade_id": "problem_234",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>datetime</th>\n",
       "      <th>obs_type</th>\n",
       "      <th>obs_value</th>\n",
       "      <th>area</th>\n",
       "      <th>datetime_dt</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751117</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>38.3</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751116</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>33.8</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-16</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751115</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>32.8</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-15</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751114</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>35.6</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751113</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>29.7</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132312</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700105</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1.1</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132313</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700104</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1.7</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132314</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700103</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>5.0</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132315</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700102</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>12.2</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132316</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>6.1</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132317 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            station  datetime obs_type  obs_value area datetime_dt  month\n",
       "0       ASN00048013  18751117     TMAX       38.3  ASN  1875-11-17     11\n",
       "1       ASN00048013  18751116     TMAX       33.8  ASN  1875-11-16     11\n",
       "2       ASN00048013  18751115     TMAX       32.8  ASN  1875-11-15     11\n",
       "3       ASN00048013  18751114     TMAX       35.6  ASN  1875-11-14     11\n",
       "4       ASN00048013  18751113     TMAX       29.7  ASN  1875-11-13     11\n",
       "...             ...       ...      ...        ...  ...         ...    ...\n",
       "132312  USW00094728  18700105     TMAX        1.1  USW  1870-01-05      1\n",
       "132313  USW00094728  18700104     TMAX        1.7  USW  1870-01-04      1\n",
       "132314  USW00094728  18700103     TMAX        5.0  USW  1870-01-03      1\n",
       "132315  USW00094728  18700102     TMAX       12.2  USW  1870-01-02      1\n",
       "132316  USW00094728  18700101     TMAX        6.1  USW  1870-01-01      1\n",
       "\n",
       "[132317 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_weather(file_name):\n",
    "    \n",
    "    \"\"\"Defines a function for loading the weatherr for a specific year\"\"\"\n",
    "    \n",
    "    file = f\"{file_name}.csv\" #Url is defined as an f-string, that loads the url, the year put into the string, is the same as the input in the function    \n",
    "    \n",
    "    df_weather = pd.read_csv(file, header=None) #Read the csv and call the dataframe df_weather\n",
    "        \n",
    "    column_names = [\"station\", \"datetime\", \"obs_type\", \"obs_value\"] #Creates a list of column names\n",
    "    df_weather.columns = column_names #rename\n",
    "    \n",
    "    df_weather[\"obs_value\"]=df_weather[\"obs_value\"] / 10 \n",
    "    \n",
    "    selection_tmax = df_weather.obs_type == \"TMAX\" #select only observations where obs_type = \"TMAX\"\n",
    "    \n",
    "    df_select = df_weather.loc[selection_tmax]\n",
    "    \n",
    "    df_sorted = df_select.sort_values(by = [\"station\", \"datetime\"], ascending = (True, False)) #sorted by station and datetime\n",
    "    \n",
    "    df_reset = df_sorted.reset_index(drop=True) #dropper det gamle index\n",
    "    \n",
    "    df_reset[\"area\"]= df_reset[\"station\"].str[:3]   #AREA\n",
    "    df_reset[\"area\"] = df_reset[\"area\"].str.replace(\"0\", \"\")\n",
    "    \n",
    "    df_reset[\"datetime_dt\"]=pd.to_datetime(df_reset[\"datetime\"], format=\"%Y%m%d\") #DATE\n",
    "    df_reset[\"month\"] = df_reset[\"datetime_dt\"].dt.month\n",
    "\n",
    "    #df_reset.set_index(\"datetime_dt\")\n",
    "    \n",
    "    df_out = df_reset.copy()\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "df_weather_period = process_weather(\"C:/Users/svanb/OneDrive/Skrivebord/sommerskole2021/assignment/weather_data_1870-1875\")\n",
    "df_weather_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df46e0f8cdbad9d34e734605451c69e6",
     "grade": true,
     "grade_id": "problem_234_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a66b53b7ae813ca45dc2578cea089c5d",
     "grade": false,
     "grade_id": "cell-7a8591d457df256a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 2.NEW (Not seen in module 2):** Try to plot the observations value of `df_weather_period` by running `df_weather_period.obs_value.plot()`. Something seems off, right? Now try to inspect the problematic subset of the dataframe by running `df_weather_period[df_weather_period.obs_value < -50]`. What can these three observations be characterized as? Drop ALL observations from the associated station from `df_weather_period`, reset the index and drop the column with the old index. Store the dataframe back into the variable `df_weather_period`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2de59076e97751d5e76fa532723f768",
     "grade": false,
     "grade_id": "problem_notseenexercises",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>datetime</th>\n",
       "      <th>obs_type</th>\n",
       "      <th>obs_value</th>\n",
       "      <th>area</th>\n",
       "      <th>datetime_dt</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751117</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>38.3</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751116</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>33.8</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-16</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751115</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>32.8</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-15</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751114</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>35.6</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751113</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>29.7</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132312</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700105</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1.1</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132313</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700104</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1.7</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132314</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700103</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>5.0</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132315</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700102</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>12.2</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132316</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>6.1</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131820 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            station  datetime obs_type  obs_value area datetime_dt  month\n",
       "0       ASN00048013  18751117     TMAX       38.3  ASN  1875-11-17     11\n",
       "1       ASN00048013  18751116     TMAX       33.8  ASN  1875-11-16     11\n",
       "2       ASN00048013  18751115     TMAX       32.8  ASN  1875-11-15     11\n",
       "3       ASN00048013  18751114     TMAX       35.6  ASN  1875-11-14     11\n",
       "4       ASN00048013  18751113     TMAX       29.7  ASN  1875-11-13     11\n",
       "...             ...       ...      ...        ...  ...         ...    ...\n",
       "132312  USW00094728  18700105     TMAX        1.1  USW  1870-01-05      1\n",
       "132313  USW00094728  18700104     TMAX        1.7  USW  1870-01-04      1\n",
       "132314  USW00094728  18700103     TMAX        5.0  USW  1870-01-03      1\n",
       "132315  USW00094728  18700102     TMAX       12.2  USW  1870-01-02      1\n",
       "132316  USW00094728  18700101     TMAX        6.1  USW  1870-01-01      1\n",
       "\n",
       "[131820 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#We first find the stations by selecting the relevant column with the corresponding station by index\n",
    "error = df_weather_period[df_weather_period['station'] == 'USW00023068' ].index\n",
    "\n",
    "#We then drop the selected stations, leaving no copy\n",
    "df_weather_period.drop(error, inplace = True)\n",
    "\n",
    "#Resets the index afterwards\n",
    "df_weather_period.reset_index(drop = True)\n",
    "\n",
    "#We can check if this works by calling the function\n",
    "##df_weather_period[df_weather_period.obs_value < -50]\n",
    "\n",
    "#New function\n",
    "df_weather_period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5971a3b2c04c14fb5fb5f180e25ff481",
     "grade": true,
     "grade_id": "problem_notseenexercises_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1b79752e5634da4d89aa3ae634563e0",
     "grade": false,
     "grade_id": "cell-c2f8ff075ab551a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 2.3.2:** \n",
    "Continuing with the `df_weather_period` from last exercise, do the following:\n",
    "> 1. Convert the `area` column to a categorical variable. \n",
    "> 2. Transform the `obs_value` column from a continuous to a categorical variable by partitioning it into `3` intervals. The first interval should contain observations with values of `obs_value` up to the 10% quantile. The second interval should contain observations with values of `obs_value` up to the 90% quantile. The third interval should contain the rest of the observations. Call this new column for `obs_value_cat`.  This can be done using the `pd.qcut()` method.\n",
    "> 3. Make another column with  `obs_value` as a categorical variable but this time label the 3 intervals as `[\"cold\", \"medium\", \"hot\"]`. This can be done by specifying the `labels` parameter in the `pd.qcut()` method of pandas. Call this new column for `obs_value_cat_labeled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a0243b6c65b39af72e8d1efead106e8",
     "grade": false,
     "grade_id": "problem_232",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>datetime</th>\n",
       "      <th>obs_type</th>\n",
       "      <th>obs_value</th>\n",
       "      <th>area</th>\n",
       "      <th>datetime_dt</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751117</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>hot</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751116</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>hot</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-16</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751115</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>hot</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-15</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751114</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>hot</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASN00048013</td>\n",
       "      <td>18751113</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>hot</td>\n",
       "      <td>ASN</td>\n",
       "      <td>1875-11-13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132312</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700105</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>medium</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132313</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700104</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>medium</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132314</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700103</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>medium</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132315</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700102</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>medium</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132316</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18700101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>medium</td>\n",
       "      <td>USW</td>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131820 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            station  datetime obs_type obs_value area datetime_dt  month\n",
       "0       ASN00048013  18751117     TMAX       hot  ASN  1875-11-17     11\n",
       "1       ASN00048013  18751116     TMAX       hot  ASN  1875-11-16     11\n",
       "2       ASN00048013  18751115     TMAX       hot  ASN  1875-11-15     11\n",
       "3       ASN00048013  18751114     TMAX       hot  ASN  1875-11-14     11\n",
       "4       ASN00048013  18751113     TMAX       hot  ASN  1875-11-13     11\n",
       "...             ...       ...      ...       ...  ...         ...    ...\n",
       "132312  USW00094728  18700105     TMAX    medium  USW  1870-01-05      1\n",
       "132313  USW00094728  18700104     TMAX    medium  USW  1870-01-04      1\n",
       "132314  USW00094728  18700103     TMAX    medium  USW  1870-01-03      1\n",
       "132315  USW00094728  18700102     TMAX    medium  USW  1870-01-02      1\n",
       "132316  USW00094728  18700101     TMAX    medium  USW  1870-01-01      1\n",
       "\n",
       "[131820 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#Converting the Area Code\n",
    "area = df_weather_period['area'].astype('category')\n",
    "\n",
    "#Added a new column with [\"cold\", \"medium\", \"hot\"] as intervals\n",
    "#Transforming the obs_value to a categorical variable\n",
    "obs_value = df_weather_period['obs_value'].astype('category')\n",
    "\n",
    "q_labels = ['cold', 'medium', 'hot']\n",
    "df_weather_period['obs_value'] = pd.qcut(df_weather_period['obs_value'], q = [0,0.10,0.90,1], labels=q_labels)\n",
    "df_weather_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed4589eb5606644a884cfc75dd36d40f",
     "grade": true,
     "grade_id": "problem_232_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0e767d450ff726563ebe1bdb729215f",
     "grade": false,
     "grade_id": "cell-77eabac0ab0cbce5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e51f2766308e96a05a0ac41d19558490",
     "grade": false,
     "grade_id": "cell-4975a2e1ab215936",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.1:** Compute the mean and median maximum daily temperature for each month-year-station pair on the dataframe `df_weather_period` from last exercise by using the _split-apply-combine_ procedure. Store the results in new columns `tmax_mean` and `tmax_median`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce92e895d0a63283094fe6c661cb5b66",
     "grade": false,
     "grade_id": "problem_331",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'year'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-980f92a5b009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#These can be stored in two variables:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdf_weather_period\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmax_mean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_weather_period\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplt_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mapply_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mdf_weather_period\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmax_median'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_weather_period\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplt_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mapply_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'median'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdf_weather_period\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   6715\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6717\u001b[1;33m         return DataFrameGroupBy(\n\u001b[0m\u001b[0;32m   6718\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6719\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[0;32m    561\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m             \u001b[1;31m# Add key to exclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'year'"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "##We start by inserting the monthly column:\n",
    "#df_weather_period['datetime'] = pd.to_datetime(df_weather_period['datetime'], format='%Y%m%d')\n",
    "#df_weather_period =df_weather_period.rename(columns={'datetime':'datetime_dt'})\n",
    "#df_weather_period['month'] =df_weather_period['datetime_dt'].dt.month\n",
    "#df_weather_period['year'] =df_weather_period['datetime_dt'].dt.year\n",
    "#df_weather_period\n",
    "\n",
    "#We then generate our split-apply-combine variables:\n",
    "splt_var=['station', 'year', 'month']\n",
    "apply_var=['obs_value']\n",
    "apply_fct = ['mean', 'median']\n",
    "\n",
    "##Rundf_weather_periodning these we get median and mean observations for each subset of the dataframes:\n",
    "#grouped = df.groupby(splt_var)[apply_var].agg(apply_fct)\n",
    "#grouped.agg(apply_fct)\n",
    "#grouped\n",
    "\n",
    "#These can be stored in two variables: \n",
    "df_weather_period['tmax_mean'] = df_weather_period.groupby(splt_var)[apply_var].transform('mean')\n",
    "df_weather_period['tmax_median'] = df_weather_period.groupby(splt_var)[apply_var].transform('median')\n",
    "df_weather_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b200933c81339b97661155bc29d76cef",
     "grade": true,
     "grade_id": "problem_331_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b08abf1e695a30158ab7d2486d7f2035",
     "grade": false,
     "grade_id": "cell-7e77713f98953bac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.2:** Plot the monthly max,min, mean, first and third quartiles for maximum temperature for the station with ID _'CA006110549'_ from `df_weather_period`.\n",
    "\n",
    "> *Hint*: the method `describe` computes all these measures. Try to make your plot look like the one below. \n",
    "<img src=\"station_data_plot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3be78d85396ff4390af4af4d247259d",
     "grade": true,
     "grade_id": "problem_332_tests",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#Define the new variable, based on the station \n",
    "desc = df_weather_period[df_weather_period['station'] == 'CA006110549']\n",
    "\n",
    "#Then we describe the meassures\n",
    "np.round(desc.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "686d2c7f6973c41533a7a27562f2df52",
     "grade": false,
     "grade_id": "cell-539af69a1ea23069",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3:** Use the station location data, which is located in this directory, to merge station locations onto `df_weather_period`. The file with station location data is called  `ghcnd-stations.txt`.  Store the result in the variable `final_data`. \n",
    "\n",
    "> _Hint:_ The location data have the folllowing format, \n",
    "\n",
    "```\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSN FLAG     73-75   Character\n",
    "HCN/CRN FLAG 77-79   Character\n",
    "WMO ID       81-85   Character\n",
    "------------------------------\n",
    "```\n",
    "\n",
    "> *Hint*: The station information has fixed width format - does there exist a pandas reader for that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80954d4aae5a86c31619d8c6a0710ae2",
     "grade": true,
     "grade_id": "cell-dbf210d3760e0e0a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#We identify the directory we work from\n",
    "from pathlib import Path\n",
    "fp = Path.cwd()/'OneDrive/Dokumenter/GitHub/isds2021/assignments/assignment1/'\n",
    "fp\n",
    "clm = ['station','Latitude', 'Longtitude', 'Elevation', 'State', 'Name', 'Gsn Flag', 'Hcn/Crn Flag', 'Wmo Id'] #Creates a list of column names\n",
    "\n",
    "#We read the stations file as fwf\n",
    "station_loc = pd.read_fwf(fp / 'ghcnd-stations.txt', names=clm)\n",
    "station_loc\n",
    "#Renaming the columns:\n",
    "#station_loc.columns = clm\n",
    "station_loc\n",
    "#We merge the station_loc onto the column 'station' and drop the 4 empty columns\n",
    "final_data = pd.merge(df_weather_period, station_loc, how = 'inner', on= 'station')\n",
    "final_data.drop(['Name', 'Gsn Flag', 'Hcn/Crn Flag', 'Wmo Id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6aa607ce0dbc9b4bfb57d56432941579",
     "grade": false,
     "grade_id": "cell-422d30deb292b4c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 4:\n",
    "\n",
    "> **Ex. 4.3.5:** This exercise consists of a set of small subelements: \n",
    ">\n",
    "> 0. Show the first five rows of the titanic dataset. What information is in the dataset?\n",
    "> 1. Use a barplot to show the probability of survival for men and women within each passenger class. \n",
    "> 2. Can you make a boxplot showing the same information (why/why not?). \n",
    "> 3. Show a boxplot for the fare-prices within each passenger class. \n",
    "> 4. Combine the two of the figures you created above into a two-panel figure and save it on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9a37823ec1f2b589e834faa96ea091d",
     "grade": true,
     "grade_id": "problem_435",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#0\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "tips = sns.load_dataset('tips')\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "\n",
    "titanic.head(5)\n",
    "#The dataset of Titanic shows all from what class the passengers are on, to age, sex and if they survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "#who survived inside each passenger class\n",
    "#make a count of each category \n",
    "f,ax = plt.subplots(1,2, figsize=(10,4))\n",
    "sns.barplot(x='class', y='survived', data=titanic, ax=ax[0]) \n",
    "\n",
    "#passenger class - survival of men and women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "sns.barplot(x='class', y='survived', hue='sex', data=titanic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Can you make a boxplot showing the same information (why/why not?).\n",
    "f,ax = plt.subplots(1,2, figsize=(10,4))\n",
    "sns.boxplot(x='class', y='sex_m', data=titanic1, ax=ax[0]) \n",
    "sns.boxplot(x='class', y='sex_f', data=titanic2, ax=ax[1])\n",
    "\n",
    "#it is not possible to make a boxplot showing the same information because the values is only 1 or 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Show a boxplot for the fare-prices within each passenger class.\n",
    "sns.boxplot(x='class', y='fare', data=titanic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "f,ax = plt.subplots(1,2, figsize=(10,4))\n",
    "sns.barplot(x='class', y='survived', hue='sex', data=titanic, ax=ax[0])\n",
    "sns.boxplot(x='class', y='fare', data=titanic, ax=ax[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77c37844cf82448bcd9e74196ca6b882",
     "grade": false,
     "grade_id": "cell-ee5aba685162b837",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 4.3.6:** Using the iris flower dataset, draw a scatterplot of sepal length and petal length. Include a second order polynomial fitted to the data. Add a title to the plot and rename the axis labels.\n",
    ">\n",
    "> _Write 3 sentences:_ Is this a meaningful way to display the data? What could we do differently?\n",
    ">\n",
    "> For a better understanding of the dataset this image might be useful:\n",
    "> <img src=\"iris_pic.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    ">\n",
    "> _Hint:_ Use the `.regplot` method from seaborn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ad886b2ef8293f79cb37b78833ff7cc",
     "grade": true,
     "grade_id": "problem_436",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='sepal_length', y='petal_length', scatter=True, order=2, data=iris)\n",
    "#maybe remove outliers for the length and width \n",
    "#Moreover we have different species of the iris flower. Might be an idea to make scatterplot for each iris species to make it more accurate. \n",
    "print(iris)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4330f62f04b07d60e818eb1893bbf82d",
     "grade": false,
     "grade_id": "cell-e6d0c56f1cf535c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 4.3.7:** Use [pairplot with hue](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to create a figure that clearly shows how the different species vary across measurements in the iris dataset. Change the color palette and remove the shading from the density plots. _Bonus:_ Try to explain how the `diag_kws` argument works (_hint:_ [read here](https://stackoverflow.com/questions/1769403/understanding-kwargs-in-python))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19e3feab810ee078ec29408d99334983",
     "grade": true,
     "grade_id": "problem_437",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set(style='ticks', color_codes=True)\n",
    "sns.pairplot(iris, hue='species', diag_kws={'linewidth':1, 'shade': False})\n",
    "#diag_kws controls the shading and lines of diagonals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4186d0ebd2c5fb68645d2d9e426d726",
     "grade": false,
     "grade_id": "cell-8302a588a83d0e73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 6\n",
    "\n",
    "> **Ex. 6.1.2.:** Use the `request` module to collect the first page of job postings and unpack the relevant `json` data into a `pandas` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8db1d5a470dcc2e13f62be3642b79ddd",
     "grade": false,
     "grade_id": "problem_612",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# Mapping exercise\n",
    "url1 = 'https://job.jobnet.dk/CV/FindWork/Search?Offset=0' #page 0 when you choose Offset=0\n",
    "response = requests.get(url1)\n",
    "read_content = response.json()\n",
    "\n",
    "job_ad_access = read_content['JobPositionPostings'] #only print dict of the job ads\n",
    "\n",
    "job_ad_access #accessing the ads\n",
    "\n",
    "#Creating the DataFrame\n",
    "df_job = pd.DataFrame(job_ad_access) \n",
    "df_job.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d697791c6f991e806f21145a7149309",
     "grade": true,
     "grade_id": "problem_612_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert sorted(df.columns) == ['Abroad', 'AnonymousEmployer', 'AssignmentStartDate', 'AutomatchType', 'Country', \n",
    "                              'DetailsUrl', 'EmploymentType', 'FormattedLastDateApplication', 'HasLocationValues', \n",
    "                              'HiringOrgCVR', 'HiringOrgName', 'ID', 'IsExternal', 'IsHotjob', 'JobAnnouncementType', \n",
    "                              'JobHeadline', 'JobLogUrl', 'JoblogWorkTime', 'LastDateApplication', 'Latitude', 'Location',\n",
    "                              'Longitude', 'Municipality', 'Occupation', 'OccupationArea', 'OccupationGroup', \n",
    "                              'OrganisationId', 'PostalCode', 'PostalCodeName', 'PostingCreated', 'Presentation',\n",
    "                              'Region', 'ShareUrl', 'Title', 'Url', 'UseWorkPlaceAddressForJoblog', 'UserLoggedIn',\n",
    "                              'Weight', 'WorkHours', 'WorkPlaceAbroad', 'WorkPlaceAddress', 'WorkPlaceCity',\n",
    "                              'WorkPlaceNotStatic', 'WorkPlaceOtherAddress', 'WorkPlacePostalCode', 'WorkplaceID']\n",
    "assert len(df) == 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f9d2b3dcba0749ece159e9c9437d75c",
     "grade": false,
     "grade_id": "cell-28466edfcc8ce716",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 6.1.3.:** How many results do you find in total? Store this number as 'TotalResultCount' for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb3de77cb24172bcf6ede32ef6ca6545",
     "grade": true,
     "grade_id": "problem_613",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "#Total amount of hits\n",
    "n_listing = read_content['TotalResultCount']\n",
    "n_listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7df6544e596d6c12679f46df0172e7b3",
     "grade": false,
     "grade_id": "cell-00d91a2dea068771",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 7\n",
    "\n",
    "> **Ex. 7.2.1:** Here we practice locating the table node of interest using the `find` method build into BeautifoulSoup. But first we have to fetch the HTML using the `requests` module. Parse the tree using `BeautifulSoup`. Next, use the **>Inspector<** tool (*right click on the table < press inspect element*) in your browser to see how to locate the Eastern Conference table node - i.e. the *tag* name of the node, and maybe some defining *attributes*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eastern Conference</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>W/L%</th>\n",
       "      <th>GB</th>\n",
       "      <th>PS/G</th>\n",
       "      <th>PA/G</th>\n",
       "      <th>SRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toronto Raptors*</td>\n",
       "      <td>59</td>\n",
       "      <td>23</td>\n",
       "      <td>.720</td>\n",
       "      <td>—</td>\n",
       "      <td>111.7</td>\n",
       "      <td>103.9</td>\n",
       "      <td>7.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston Celtics*</td>\n",
       "      <td>55</td>\n",
       "      <td>27</td>\n",
       "      <td>.671</td>\n",
       "      <td>4.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>100.4</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Philadelphia 76ers*</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>.634</td>\n",
       "      <td>7.0</td>\n",
       "      <td>109.8</td>\n",
       "      <td>105.3</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cleveland Cavaliers*</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>.610</td>\n",
       "      <td>9.0</td>\n",
       "      <td>110.9</td>\n",
       "      <td>109.9</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indiana Pacers*</td>\n",
       "      <td>48</td>\n",
       "      <td>34</td>\n",
       "      <td>.585</td>\n",
       "      <td>11.0</td>\n",
       "      <td>105.6</td>\n",
       "      <td>104.2</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Miami Heat*</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>.537</td>\n",
       "      <td>15.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>102.9</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Milwaukee Bucks*</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>.537</td>\n",
       "      <td>15.0</td>\n",
       "      <td>106.5</td>\n",
       "      <td>106.8</td>\n",
       "      <td>-0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Washington Wizards*</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>.524</td>\n",
       "      <td>16.0</td>\n",
       "      <td>106.6</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "      <td>.476</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103.8</td>\n",
       "      <td>103.9</td>\n",
       "      <td>-0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Charlotte Hornets</td>\n",
       "      <td>36</td>\n",
       "      <td>46</td>\n",
       "      <td>.439</td>\n",
       "      <td>23.0</td>\n",
       "      <td>108.2</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>29</td>\n",
       "      <td>53</td>\n",
       "      <td>.354</td>\n",
       "      <td>30.0</td>\n",
       "      <td>104.5</td>\n",
       "      <td>108.0</td>\n",
       "      <td>-3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Brooklyn Nets</td>\n",
       "      <td>28</td>\n",
       "      <td>54</td>\n",
       "      <td>.341</td>\n",
       "      <td>31.0</td>\n",
       "      <td>106.6</td>\n",
       "      <td>110.3</td>\n",
       "      <td>-3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "      <td>.329</td>\n",
       "      <td>32.0</td>\n",
       "      <td>102.9</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-6.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Orlando Magic</td>\n",
       "      <td>25</td>\n",
       "      <td>57</td>\n",
       "      <td>.305</td>\n",
       "      <td>34.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>108.2</td>\n",
       "      <td>-4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Atlanta Hawks</td>\n",
       "      <td>24</td>\n",
       "      <td>58</td>\n",
       "      <td>.293</td>\n",
       "      <td>35.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>108.8</td>\n",
       "      <td>-5.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Eastern Conference   W   L  W/L%    GB   PS/G   PA/G    SRS\n",
       "0       Toronto Raptors*  59  23  .720     —  111.7  103.9   7.29\n",
       "1        Boston Celtics*  55  27  .671   4.0  104.0  100.4   3.23\n",
       "2    Philadelphia 76ers*  52  30  .634   7.0  109.8  105.3   4.30\n",
       "3   Cleveland Cavaliers*  50  32  .610   9.0  110.9  109.9   0.59\n",
       "4        Indiana Pacers*  48  34  .585  11.0  105.6  104.2   1.18\n",
       "5            Miami Heat*  44  38  .537  15.0  103.4  102.9   0.15\n",
       "6       Milwaukee Bucks*  44  38  .537  15.0  106.5  106.8  -0.45\n",
       "7    Washington Wizards*  43  39  .524  16.0  106.6  106.0   0.53\n",
       "8        Detroit Pistons  39  43  .476  20.0  103.8  103.9  -0.26\n",
       "9      Charlotte Hornets  36  46  .439  23.0  108.2  108.0   0.07\n",
       "10       New York Knicks  29  53  .354  30.0  104.5  108.0  -3.53\n",
       "11         Brooklyn Nets  28  54  .341  31.0  106.6  110.3  -3.67\n",
       "12         Chicago Bulls  27  55  .329  32.0  102.9  110.0  -6.84\n",
       "13         Orlando Magic  25  57  .305  34.0  103.4  108.2  -4.92\n",
       "14         Atlanta Hawks  24  58  .293  35.0  103.4  108.8  -5.30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import selenium\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "url =  'https://www.basketball-reference.com/leagues/NBA_2018.html'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "soup.find_all('th')[0].text.strip()\n",
    "\n",
    "\n",
    "Eastern_Conference= []\n",
    "W = []\n",
    "L= []\n",
    "W_L=[]\n",
    "GB=[]\n",
    "PSG=[]\n",
    "PAG=[]\n",
    "SRS=[]\n",
    "\n",
    "for i in range(15):\n",
    "    Eastern_Conference.append(soup.find_all('th',{'class':'left'})[i+1].text.strip())\n",
    "    W.append(soup.find_all('td',{'data-stat':'wins'})[i].text.strip())\n",
    "    L.append(soup.find_all('td',{'data-stat':'losses'})[i].text.strip())\n",
    "    W_L.append(soup.find_all('td',{'data-stat':'win_loss_pct'})[i].text.strip())\n",
    "    GB.append(soup.find_all('td',{'data-stat':'gb'})[i].text.strip())\n",
    "    PSG.append(soup.find_all('td',{'data-stat':'pts_per_g'})[i].text.strip())\n",
    "    PAG.append(soup.find_all('td',{'data-stat':'opp_pts_per_g'})[i].text.strip())\n",
    "    SRS.append(soup.find_all('td',{'data-stat':'srs'})[i].text.strip())\n",
    "    \n",
    "\n",
    "df = pd.DataFrame({'Eastern Conference':Eastern_Conference, 'W':W, 'L':L, 'W/L%':W_L, 'GB':GB, 'PS/G':PSG, 'PA/G':PAG, 'SRS':SRS})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eff0b1c57b8bec049808e5b2f0f50bd8",
     "grade": true,
     "grade_id": "problem_721",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Conference Standings \\n* Playoff teams\\n\\n\\n\\n\\n\\nConference Standings Table\\n\\n\\n\\nEastern Conference\\nW\\nL\\nW/L%\\nGB\\nPS/G\\nPA/G\\nSRS\\n\\n\\nToronto Raptors*5923.720—111.7103.97.29\\nBoston Celtics*5527.6714.0104.0100.43.23\\nPhiladelphia 76ers*5230.6347.0109.8105.34.30\\nCleveland Cavaliers*5032.6109.0110.9109.90.59\\nIndiana Pacers*4834.58511.0105.6104.21.18\\nMiami Heat*4438.53715.0103.4102.90.15\\nMilwaukee Bucks*4438.53715.0106.5106.8-0.45\\nWashington Wizards*4339.52416.0106.6106.00.53\\nDetroit Pistons3943.47620.0103.8103.9-0.26\\nCharlotte Hornets3646.43923.0108.2108.00.07\\nNew York Knicks2953.35430.0104.5108.0-3.53\\nBrooklyn Nets2854.34131.0106.6110.3-3.67\\nChicago Bulls2755.32932.0102.9110.0-6.84\\nOrlando Magic2557.30534.0103.4108.2-4.92\\nAtlanta Hawks2458.29335.0103.4108.8-5.30'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import selenium\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "url =  'https://www.basketball-reference.com/leagues/NBA_2018.html'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "soup.find_all('div',{'class':'table_wrapper'})[0].text.strip()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
